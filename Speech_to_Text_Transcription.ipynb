{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Audio - Whisper (Speech-to-Text) \n",
    "# Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univeral Code Used for the Entire Notebook\n",
    "\n",
    "Let's set up our libraries and client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI  # For OpenAI API and event handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "client = OpenAI()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transcription\n",
    "#### Request Body\n",
    "\n",
    "**file** `file`  **Required**  \n",
    "The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
    "\n",
    "**model** `string`  **Required**  \n",
    "ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available.\n",
    "\n",
    "**language** `string`  **Optional**  \n",
    "The language of the input audio. Supplying the input language in **ISO-639-1** format will improve accuracy and latency.\n",
    "\n",
    "**prompt** `string`  **Optional**  \n",
    "An optional text to guide the model's style or continue a previous audio segment. The **prompt** should match the audio language.\n",
    "\n",
    "**response_format** `string`  **Optional** Defaults to json  \n",
    "The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.\n",
    "\n",
    "**temperature** `number`  **Optional** Defaults to 0  \n",
    "The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use **log probability** to automatically increase the temperature until certain thresholds are hit.\n",
    "\n",
    "**timestamp_granularities[]** `array`  **Optional** Defaults to segment  \n",
    "The timestamp granularities to populate for this transcription. **response_format** must be set to `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. **Note**: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcript)\n",
    "print(\"\\n\\n\")\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.0,\n",
    "    timestamp_granularities=[\"segment\"],\n",
    ")\n",
    "\n",
    "print(transcript)\n",
    "print(\"\\n\\n\")\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verbose_json with segment timestamp granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"verbose_json\",\n",
    "    temperature=0.0,\n",
    "    timestamp_granularities=[\"segment\"],\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verbose_json\n",
    "This transcription is a detailed output from a speech-to-text model that transcribed President Franklin D. Roosevelt's speech delivered on December 8, 1941, following the attack on Pearl Harbor. Hereâ€™s a breakdown of the key components in the transcription data:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Text**:\n",
    "   - This is the main body of the transcribed speech text. It captures the entirety of the speech delivered by Roosevelt, describing the events of the Japanese attack on Pearl Harbor, the resulting damage, and the United States' response.\n",
    "\n",
    "2. **Task**:\n",
    "   - The task is labeled as `transcribe`, indicating the primary function was to convert spoken words from an audio file into text.\n",
    "\n",
    "3. **Language**:\n",
    "   - The transcription was identified as being in `english`.\n",
    "\n",
    "4. **Duration**:\n",
    "   - The duration of the audio file is given as `520.4299926757812` seconds, indicating the length of the audio that was transcribed.\n",
    "\n",
    "5. **Segments**:\n",
    "   - The transcription is broken down into segments, each of which includes:\n",
    "     - **ID**: A unique identifier for each segment.\n",
    "     - **Seek**: The point in the audio where this segment begins.\n",
    "     - **Start** and **End**: The timestamps for the beginning and end of each segment.\n",
    "     - **Text**: The transcribed text for that particular segment.\n",
    "     - **Tokens**: Encoded representations of the segment text used by the model.\n",
    "     - **Temperature**: The model's temperature setting, which affects randomness in text generation.\n",
    "     - **Avg_logprob**: The average log probability of the tokens, indicating the confidence of the transcription.\n",
    "     - **Compression_ratio**: The ratio indicating the compression level applied.\n",
    "     - **No_speech_prob**: The probability that no speech was detected in this segment.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. **Main Text**:\n",
    "   - The transcribed text is a historical speech by Roosevelt, informing Congress about the attack on Pearl Harbor by Japan. It details the events, the diplomatic context, and calls for action against Japan.\n",
    "\n",
    "2. **Task and Language**:\n",
    "   - The model's task was to transcribe spoken English from an audio file, accurately converting it into written text.\n",
    "\n",
    "3. **Segments**:\n",
    "   - Each segment represents a portion of the speech. Breaking down the text into segments helps manage large transcriptions and allows for detailed analysis of each part.\n",
    "\n",
    "4. **Metadata in Segments**:\n",
    "   - **ID** and **Seek** help in tracking and locating specific parts of the audio.\n",
    "   - **Start** and **End** times define the exact timing for each segment in the audio.\n",
    "   - **Tokens** are used internally by the model to process and generate text.\n",
    "   - **Temperature** controls the randomness; a setting of `0.0` means the model outputs are deterministic.\n",
    "   - **Avg_logprob** provides insight into the model's confidence; lower values generally indicate higher confidence.\n",
    "   - **Compression_ratio** gives an idea of the text's density or how much information is packed into the segment.\n",
    "   - **No_speech_prob** indicates the likelihood that the segment contains no speech, which is useful for identifying silent or non-speech segments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verbose_json with word timestamp granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"verbose_json\",\n",
    "    temperature=0.0,\n",
    "    timestamp_granularities=[\"word\"],\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verbose_json with word timestamp granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Segments vs. Words Metadata**:\n",
    "   - **First Transcription**: Contains metadata broken into `segments`, each with attributes like `id`, `seek`, `start`, `end`, `text`, `tokens`, `temperature`, `avg_logprob`, `compression_ratio`, and `no_speech_prob`.\n",
    "   - **Second Transcription**: Contains metadata broken into individual `words`, each with `word`, `start`, and `end` attributes.\n",
    "\n",
    "2. **Segment Attributes (First Transcription)**:\n",
    "   - **id**: Unique identifier for each segment.\n",
    "   - **seek**: The point in the audio where the segment begins.\n",
    "   - **start** and **end**: Timestamps for the beginning and end of each segment.\n",
    "   - **text**: The transcribed text for the segment.\n",
    "   - **tokens**: Encoded representations of the segment text used by the model.\n",
    "   - **temperature**: The model's temperature setting, affecting randomness in text generation.\n",
    "   - **avg_logprob**: The average log probability of the tokens, indicating the confidence of the transcription.\n",
    "   - **compression_ratio**: The ratio indicating the compression level applied.\n",
    "   - **no_speech_prob**: Probability that no speech was detected in the segment.\n",
    "\n",
    "3. **Word Attributes (Second Transcription)**:\n",
    "   - **word**: The individual word transcribed.\n",
    "   - **start** and **end**: Timestamps for the start and end of each word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"text\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### srt (SubRip Subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"srt\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vtt (Web Video Text Tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"vtt\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/acronym_audio.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/acronym_audio.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"GPT-4o, GPT-4o mini\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting with Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Umm, I think we should ahhh, do this thing.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/temperature_audio_test.mp4\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Let us discuss the topics of the day.\",\n",
    "    response_format=\"json\",\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transcription of the audio file\n",
    "audio_file = open(\"./artifacts/fdr_speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    language=\"en\",\n",
    "    prompt=\"Transcribe the following audio file.\",\n",
    "    response_format=\"text\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(transcript)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be given a transcript of an audio file. Your task is to look at it and give me any corrections you think need to be done\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": transcript\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
